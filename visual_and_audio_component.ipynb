{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Relevant Libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models import vgg16, densenet121, resnet152\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# change to GPU\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization parameters for pre-trained PyTorch models\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# dataset loader which takes in the frames of the video\n",
    "# default number of frames is 40, padding with the first frame if the video does not have enough frames\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self, dataset_path, split_path, split_number, input_shape, sequence_length, training):\n",
    "        super().__init__()\n",
    "        self.training = training\n",
    "        self.label_index = self._extract_label_mapping(split_path)\n",
    "        self.sequences = self._extract_sequence_paths(dataset_path, split_path, split_number, training)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.label_names = sorted(list(set([self._activity_from_path(seq_path) for seq_path in self.sequences])))\n",
    "        self.num_classes = len(self.label_names)\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(input_shape[-2:], Image.BICUBIC),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ]\n",
    "        )\n",
    "        with open('audio_features.pickle', 'rb') as handle:\n",
    "            self.audio_features = pickle.load(handle)\n",
    "\n",
    "    def _extract_label_mapping(self, split_path=\"train_test_split\"):\n",
    "        \"\"\" Extracts a mapping between activity name and softmax index \"\"\"\n",
    "        with open(os.path.join(split_path, \"classInd.txt\")) as file:\n",
    "            lines = file.read().splitlines()\n",
    "        label_mapping = {}\n",
    "        for line in lines:\n",
    "            label, action = line.split()\n",
    "            label_mapping[action] = int(label) - 1\n",
    "        return label_mapping\n",
    "\n",
    "    def _extract_sequence_paths(\n",
    "        self, dataset_path, split_path=\"train_test_split\", split_number=1, training=True\n",
    "    ):\n",
    "        \"\"\" Extracts paths to sequences given the specified train / test split \"\"\"\n",
    "        assert split_number in [1, 2, 3], \"Split number has to be one of {1, 2, 3}\"\n",
    "        fn = f\"trainlist0{split_number}.txt\" if training else f\"testlist0{split_number}.txt\"\n",
    "        split_path = os.path.join(split_path, fn)\n",
    "        with open(split_path) as file:\n",
    "            lines = file.read().splitlines()\n",
    "        sequence_paths = []\n",
    "        for line in lines:\n",
    "            seq_name = line.split(\".avi\")[0]\n",
    "            sequence_paths += [os.path.join(dataset_path, seq_name)]\n",
    "        return sequence_paths\n",
    "\n",
    "    def _activity_from_path(self, path):\n",
    "        \"\"\" Extracts activity name from filepath \"\"\"\n",
    "        return path.split(\"/\")[-2]\n",
    "\n",
    "    def _frame_number(self, image_path):\n",
    "        \"\"\" Extracts frame number from filepath \"\"\"\n",
    "        return int(image_path.split(\"/\")[-1].split(\".jpg\")[0])\n",
    "\n",
    "    def _pad_to_length(self, sequence):\n",
    "        \"\"\" Pads the sequence to required sequence length \"\"\"\n",
    "        left_pad = sequence[0]\n",
    "        if self.sequence_length is not None:\n",
    "            while len(sequence) < self.sequence_length:\n",
    "                sequence.insert(0, left_pad)\n",
    "        return sequence\n",
    "    \n",
    "    def _get_audio_features(self, sequence):\n",
    "        \"\"\" Get the audio features from the pickle \"\"\"\n",
    "        # Hardcoded\n",
    "    \n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sequence_path = self.sequences[index % len(self)]\n",
    "        image_name =  sequence_path.split('/')[-1]+\".wav\"\n",
    "        # print(image_name)\n",
    "        audio = self.audio_features[image_name]\n",
    "        audio = torch.Tensor(audio)\n",
    "        # Sort frame sequence based on frame number\n",
    "        image_paths = sorted(glob.glob(f\"{sequence_path}/*.jpg\"), key=lambda path: self._frame_number(path))\n",
    "        # Pad frames sequences shorter than `self.sequence_length` to length\n",
    "        image_paths = self._pad_to_length(image_paths)\n",
    "        if self.training:\n",
    "            # Randomly choose sample interval and start frame\n",
    "            sample_interval = np.random.randint(1, len(image_paths) // self.sequence_length + 1)\n",
    "            start_i = np.random.randint(0, len(image_paths) - sample_interval * self.sequence_length + 1)\n",
    "            flip = np.random.random() < 0.5\n",
    "        else:\n",
    "            # Start at first frame and sample uniformly over sequence\n",
    "            start_i = 0\n",
    "            sample_interval = 1 if self.sequence_length is None else len(image_paths) // self.sequence_length\n",
    "            flip = False\n",
    "        # Extract frames as tensors\n",
    "        image_sequence = []\n",
    "        for i in range(start_i, len(image_paths), sample_interval):\n",
    "            if self.sequence_length is None or len(image_sequence) < self.sequence_length:\n",
    "                image_tensor = self.transform(Image.open(image_paths[i]))\n",
    "                if flip:\n",
    "                    image_tensor = torch.flip(image_tensor, (-1,))\n",
    "                image_sequence.append(image_tensor)\n",
    "        image_sequence = torch.stack(image_sequence)\n",
    "        target = self.label_index[self._activity_from_path(sequence_path)]\n",
    "        \n",
    "        \n",
    "        return image_sequence, target, audio\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "#         Encoder\n",
    "##############################\n",
    "\n",
    "\n",
    "# encoder for visual frames\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        # for resnet encoder, use the following:\n",
    "        # resnet = resnet152(pretrained=True)\n",
    "        # self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        # self.final = nn.Sequential(\n",
    "        #     nn.Linear(resnet.fc.in_features, latent_dim), nn.BatchNorm1d(latent_dim, momentum=0.01)\n",
    "        # )\n",
    "        \n",
    "        # for densenet encoder, use the following:\n",
    "        # model = densenet161(pretrained=True)\n",
    "        # self.feature_extractor = nn.Sequential(*list(model.children())[:-1])\n",
    "        # self.final = nn.Sequential(\n",
    "        #     nn.Linear(108192, latent_dim), nn.BatchNorm1d(latent_dim, momentum=0.01)\n",
    "        # )\n",
    "\n",
    "        # for vgg16 encoder, using the following: \n",
    "        vgg = vgg16(pretrained=True) \n",
    "        self.feature_extractor = nn.Sequential(*list(vgg.children())[:-1])\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(vgg.classifier[6].in_features, latent_dim), nn.BatchNorm1d(latent_dim, momentum=0.01)\n",
    "        )\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(25088, latent_dim), nn.BatchNorm1d(latent_dim, momentum=0.01)\n",
    "        )\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.feature_extractor(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.final(x)\n",
    "\n",
    "    \n",
    "##############################\n",
    "#           LSTM\n",
    "##############################\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, latent_dim, num_layers, hidden_dim, bidirectional):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(latent_dim, hidden_dim, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.hidden_state = None\n",
    "\n",
    "    def reset_hidden_state(self):\n",
    "        self.hidden_state = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, self.hidden_state = self.lstm(x, self.hidden_state)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "##############################\n",
    "#         ConvLSTM\n",
    "##############################\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes, latent_dim=512, lstm_layers=1, hidden_dim=1024, bidirectional=True, attention=True\n",
    "    ):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.lstm = LSTM(latent_dim, lstm_layers, hidden_dim, bidirectional)\n",
    "        self.output_layers = nn.Sequential(\n",
    "            nn.Linear((2 * hidden_dim if bidirectional else hidden_dim)+609, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim, momentum=0.01),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "        self.attention = attention\n",
    "        self.attention_layer = nn.Linear(2 * hidden_dim if bidirectional else hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, audio):\n",
    "        batch_size, seq_length, c, h, w = x.shape\n",
    "        x = x.view(batch_size * seq_length, c, h, w)\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(batch_size, seq_length, -1)\n",
    "        x = self.lstm(x)\n",
    "        if self.attention:\n",
    "            attention_w = F.softmax(self.attention_layer(x).squeeze(-1), dim=-1)\n",
    "            x = torch.sum(attention_w.unsqueeze(-1) * x, dim=1)\n",
    "        else:\n",
    "            x = x[:, -1]\n",
    "        # concatenate audio features with visual features\n",
    "        x = torch.cat((x, audio), dim=1)\n",
    "        return self.output_layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training parameters\n",
    "\n",
    "image_shape = (3, 224, 224) # (opt.channels, opt.img_dim, opt.img_dim)\n",
    "dataset_path = \"UCF_49-frames\"\n",
    "split_path = \"train_test_split\"\n",
    "split_number = 1\n",
    "sequence_length = 50\n",
    "batch_size = 4\n",
    "latent_dim = 256\n",
    "num_epochs = 15\n",
    "checkpoint_model = \"\"\n",
    "checkpoint_interval = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training set\n",
    "train_dataset = Dataset(\n",
    "    dataset_path=dataset_path,\n",
    "    split_path=split_path,\n",
    "    split_number=split_number,\n",
    "    input_shape=image_shape,\n",
    "    sequence_length=sequence_length,\n",
    "    training=True\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# Define test set\n",
    "test_dataset = Dataset(\n",
    "    dataset_path=dataset_path,\n",
    "    split_path=split_path,\n",
    "    split_number=split_number,\n",
    "    input_shape=image_shape,\n",
    "    sequence_length=sequence_length,\n",
    "    training=False\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Classification criterion\n",
    "cls_criterion = nn.CrossEntropyLoss().to(device)\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define network\n",
    "model = ConvLSTM(\n",
    "    num_classes=train_dataset.num_classes,\n",
    "    latent_dim=latent_dim,\n",
    "    lstm_layers=1,\n",
    "    hidden_dim=1024,\n",
    "    bidirectional=True,\n",
    "    attention=True,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "# summary(model, input_size=(50, 3, 224, 224), batch_size=4)\n",
    "        \n",
    "        \n",
    "# Add weights from checkpoint model if specified\n",
    "if checkpoint_model:\n",
    "    model.load_state_dict(torch.load(opt.checkpoint_model))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(epoch):\n",
    "    \"\"\" Evaluate the model on the test set \"\"\"\n",
    "    model.eval()\n",
    "    test_metrics = {\"loss\": [], \"acc\": []}\n",
    "    for images, labels, audio in tqdm(test_dataloader):\n",
    "        image_sequences = Variable(images.to(device), requires_grad=False)\n",
    "        audio_sequences = Variable(audio.to(device), requires_grad=False)\n",
    "        labels = Variable(labels, requires_grad=False).to(device)\n",
    "        with torch.no_grad():\n",
    "            # Reset LSTM hidden state\n",
    "            model.lstm.reset_hidden_state()\n",
    "            # Get sequence predictions\n",
    "            predictions = model(image_sequences, audio_sequences)\n",
    "        # Compute metrics\n",
    "        acc = 100 * (predictions.detach().argmax(1) == labels).cpu().numpy().mean()\n",
    "        loss = cls_criterion(predictions, labels).item()\n",
    "        # Keep track of loss and accuracy\n",
    "        test_metrics[\"loss\"].append(loss)\n",
    "        test_metrics[\"acc\"].append(acc)\n",
    "    model.train()\n",
    "    return test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile = 'log_densenet161_with_audio.txt'\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_metrics = {\"loss\": [], \"acc\": []}\n",
    "    prev_time = time.time()\n",
    "    with open(logfile, 'a') as log: \n",
    "        log.write(f\"--- Epoch {epoch} ---\\n\")\n",
    "        log.close()\n",
    "    print(f\"--- Epoch {epoch} ---\")\n",
    "    for images, labels, audio in tqdm(train_dataloader):\n",
    "        if images.size(0) == 1:\n",
    "            continue\n",
    "\n",
    "        image_sequences = Variable(images.to(device), requires_grad=True)\n",
    "        audio_sequences = Variable(audio.to(device), requires_grad=True)\n",
    "        labels = Variable(labels.to(device), requires_grad=False)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Reset LSTM hidden state\n",
    "        model.lstm.reset_hidden_state()\n",
    "\n",
    "        # Get sequence predictions\n",
    "        predictions = model(image_sequences, audio_sequences)\n",
    "\n",
    "        # Compute metrics\n",
    "        loss = cls_criterion(predictions, labels)\n",
    "        acc = 100 * (predictions.detach().argmax(1) == labels).cpu().numpy().mean()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Keep track of epoch metrics\n",
    "        epoch_metrics[\"loss\"].append(loss.item())\n",
    "        epoch_metrics[\"acc\"].append(acc)\n",
    "#         print(loss.item())\n",
    "#         print(acc)\n",
    "\n",
    "        # Empty cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_metrics = test_model(epoch)\n",
    "    \n",
    "    with open(logfile, 'a') as log: \n",
    "        with open(logfile, 'a') as log: \n",
    "            log.write(\"Epoch: {}/{} - \".format(epoch, num_epochs))\n",
    "            log.write(\"Training Loss: {:.3f} ({:.3f}) \".format(epoch_metrics[\"loss\"][-1], np.mean(epoch_metrics[\"loss\"])))\n",
    "            log.write(\"Training Acc: {:.3f} ({:.3f}) \".format(epoch_metrics[\"acc\"][-1], np.mean(epoch_metrics[\"acc\"])))\n",
    "            log.write(\"Test Loss: {:.3f} ({:.3f}) \".format(test_metrics[\"loss\"][-1], np.mean(test_metrics[\"loss\"])))\n",
    "            log.write(\"Test Acc: {:.3f} ({:.3f})\\n\".format(test_metrics[\"acc\"][-1], np.mean(test_metrics[\"acc\"])))\n",
    "    \n",
    "    print(\"Epoch: {}/{} - \".format(epoch, num_epochs),\n",
    "          \"Training Loss: {:.3f} ({:.3f})\".format(epoch_metrics[\"loss\"][-1], np.mean(epoch_metrics[\"loss\"])),\n",
    "          \"Training Acc: {:.3f} ({:.3f})\".format(epoch_metrics[\"acc\"][-1], np.mean(epoch_metrics[\"acc\"])),\n",
    "          \"Test Loss: {:.3f} ({:.3f})\".format(test_metrics[\"loss\"][-1], np.mean(test_metrics[\"loss\"])),\n",
    "          \"Test Acc: {:.3f} ({:.3f})\".format(test_metrics[\"acc\"][-1], np.mean(test_metrics[\"acc\"])))\n",
    "\n",
    "    # Save model checkpoint\n",
    "    if epoch % checkpoint_interval == 0:\n",
    "        os.makedirs(\"model_checkpoints\", exist_ok=True)\n",
    "        torch.save(model.state_dict(), f\"model_checkpoints/{model.__class__.__name__}_{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph_epoch(train_loss, val_loss, epoch):\n",
    "    plt.title(\"Epoch {} Training/Validation losses over epoch\".format(epoch+1))\n",
    "    plt.plot(train_loss, label = \"Training Loss\")\n",
    "    plt.plot(val_loss, label = 'Validation Loss')\n",
    "    plt.xlabel('Number of epoch', fontsize = 14)\n",
    "    plt.ylabel('Loss', fontsize = 14)\n",
    "    plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Relevant Libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models import vgg16, densenet161\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# change to GPU\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization parameters for pre-trained PyTorch models\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "# dataset loader which takes in the frames of the video\n",
    "# default number of frames is 40, padding with the first frame if the video does not have enough frames\n",
    "class Dataset():\n",
    "    def __init__(self, dataset_path, split_path, split_number, input_shape, sequence_length, training):\n",
    "        super().__init__()\n",
    "        self.training = training\n",
    "        self.label_index = self._extract_label_mapping(split_path)\n",
    "        self.sequences = self._extract_sequence_paths(dataset_path, split_path, split_number, training)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.label_names = sorted(list(set([self._activity_from_path(seq_path) for seq_path in self.sequences])))\n",
    "        self.num_classes = len(self.label_names)\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(input_shape[-2:], Image.BICUBIC),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _extract_label_mapping(self, split_path=\"data/train_test_split\"):\n",
    "        \"\"\" Extracts a mapping between activity name and softmax index \"\"\"\n",
    "        with open(os.path.join(split_path, \"classInd.txt\")) as file:\n",
    "            lines = file.read().splitlines()\n",
    "        label_mapping = {}\n",
    "        for line in lines:\n",
    "            label, action = line.split()\n",
    "            label_mapping[action] = int(label) - 1\n",
    "        return label_mapping\n",
    "\n",
    "    def _extract_sequence_paths(\n",
    "        self, dataset_path, split_path=\"data/train_test_split\", split_number=1, training=True\n",
    "    ):\n",
    "        \"\"\" Extracts paths to sequences given the specified train / test split \"\"\"\n",
    "        assert split_number in [1, 2, 3], \"Split number has to be one of {1, 2, 3}\"\n",
    "        fn = f\"trainlist0{split_number}.txt\" if training else f\"testlist0{split_number}.txt\"\n",
    "        split_path = os.path.join(split_path, fn)\n",
    "        with open(split_path) as file:\n",
    "            lines = file.read().splitlines()\n",
    "        sequence_paths = []\n",
    "        for line in lines:\n",
    "            seq_name = line.split(\".avi\")[0]\n",
    "            sequence_paths += [os.path.join(dataset_path, seq_name)]\n",
    "        return sequence_paths\n",
    "\n",
    "    def _activity_from_path(self, path):\n",
    "        \"\"\" Extracts activity name from filepath \"\"\"\n",
    "        return path.split(\"/\")[-2]\n",
    "\n",
    "    def _frame_number(self, image_path):\n",
    "        \"\"\" Extracts frame number from filepath \"\"\"\n",
    "        return int(image_path.split(\"/\")[-1].split(\".jpg\")[0])\n",
    "\n",
    "    def _pad_to_length(self, sequence):\n",
    "        \"\"\" Pads the sequence to required sequence length \"\"\"\n",
    "        left_pad = sequence[0]\n",
    "        if self.sequence_length is not None:\n",
    "            while len(sequence) < self.sequence_length:\n",
    "                sequence.insert(0, left_pad)\n",
    "        return sequence\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sequence_path = self.sequences[index % len(self)]\n",
    "        # Sort frame sequence based on frame number\n",
    "        image_paths = sorted(glob.glob(f\"{sequence_path}/*.jpg\"), key=lambda path: self._frame_number(path))\n",
    "        # Pad frames sequences shorter than `self.sequence_length` to length\n",
    "        image_paths = self._pad_to_length(image_paths)\n",
    "        if self.training:\n",
    "            # Randomly choose sample interval and start frame\n",
    "            sample_interval = np.random.randint(1, len(image_paths) // self.sequence_length + 1)\n",
    "            start_i = np.random.randint(0, len(image_paths) - sample_interval * self.sequence_length + 1)\n",
    "            flip = np.random.random() < 0.5\n",
    "        else:\n",
    "            # Start at first frame and sample uniformly over sequence\n",
    "            start_i = 0\n",
    "            sample_interval = 1 if self.sequence_length is None else len(image_paths) // self.sequence_length\n",
    "            flip = False\n",
    "        # Extract frames as tensors\n",
    "        image_sequence = []\n",
    "        for i in range(start_i, len(image_paths), sample_interval):\n",
    "            if self.sequence_length is None or len(image_sequence) < self.sequence_length:\n",
    "                image_tensor = self.transform(Image.open(image_paths[i]))\n",
    "                if flip:\n",
    "                    image_tensor = torch.flip(image_tensor, (-1,))\n",
    "                image_sequence.append(image_tensor)\n",
    "        image_sequence = torch.stack(image_sequence)\n",
    "        target = self.label_index[self._activity_from_path(sequence_path)]\n",
    "        return image_sequence, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "#         Encoder\n",
    "##############################\n",
    "\n",
    "# encoder for visual frames\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # for resnet encoder, use the following:\n",
    "        # resnet = resnet152(pretrained=True)\n",
    "        # self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        # self.final = nn.Sequential(\n",
    "        #     nn.Linear(resnet.fc.in_features, latent_dim), nn.BatchNorm1d(latent_dim, momentum=0.01)\n",
    "        # )\n",
    "        \n",
    "        # for densenet encoder, use the following:\n",
    "        # model = densenet161(pretrained=True)\n",
    "        # self.feature_extractor = nn.Sequential(*list(model.children())[:-1])\n",
    "        # self.final = nn.Sequential(\n",
    "        #     nn.Linear(108192, latent_dim), nn.BatchNorm1d(latent_dim, momentum=0.01)\n",
    "        # )\n",
    "\n",
    "        # for vgg16 encoder, using the following: \n",
    "        vgg = vgg16(pretrained=True) \n",
    "        self.feature_extractor = nn.Sequential(*list(vgg.children())[:-1])\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(vgg.classifier[6].in_features, latent_dim), nn.BatchNorm1d(latent_dim, momentum=0.01)\n",
    "        )\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(25088, latent_dim), nn.BatchNorm1d(latent_dim, momentum=0.01)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.feature_extractor(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.final(x)\n",
    "\n",
    "    \n",
    "##############################\n",
    "#           LSTM\n",
    "##############################\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, latent_dim, num_layers, hidden_dim, bidirectional):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(latent_dim, hidden_dim, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.hidden_state = None\n",
    "\n",
    "    def reset_hidden_state(self):\n",
    "        self.hidden_state = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, self.hidden_state = self.lstm(x, self.hidden_state)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "##############################\n",
    "#         ConvLSTM\n",
    "##############################\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes, latent_dim=512, lstm_layers=1, hidden_dim=1024, bidirectional=True, attention=True\n",
    "    ):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.lstm = LSTM(latent_dim, lstm_layers, hidden_dim, bidirectional)\n",
    "        self.output_layers = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim if bidirectional else hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim, momentum=0.01),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "        self.attention = attention\n",
    "        self.attention_layer = nn.Linear(2 * hidden_dim if bidirectional else hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, c, h, w = x.shape\n",
    "        x = x.view(batch_size * seq_length, c, h, w)\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(batch_size, seq_length, -1)\n",
    "        x = self.lstm(x)\n",
    "        if self.attention:\n",
    "            attention_w = F.softmax(self.attention_layer(x).squeeze(-1), dim=-1)\n",
    "            x = torch.sum(attention_w.unsqueeze(-1) * x, dim=1)\n",
    "        else:\n",
    "            x = x[:, -1]\n",
    "        return self.output_layers(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training parameters\n",
    "\n",
    "image_shape = (3, 224, 224) # (opt.channels, opt.img_dim, opt.img_dim)\n",
    "dataset_path = \"UCF_49-frames\"\n",
    "split_path = \"train_test_split\"\n",
    "split_number = 1\n",
    "sequence_length = 50\n",
    "batch_size = 4\n",
    "latent_dim = 256\n",
    "num_epochs = 15\n",
    "checkpoint_model = \"\"\n",
    "checkpoint_interval = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training set\n",
    "train_dataset = Dataset(\n",
    "    dataset_path=dataset_path,\n",
    "    split_path=split_path,\n",
    "    split_number=split_number,\n",
    "    input_shape=image_shape,\n",
    "    sequence_length=sequence_length,\n",
    "    training=True\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# Define test set\n",
    "test_dataset = Dataset(\n",
    "    dataset_path=dataset_path,\n",
    "    split_path=split_path,\n",
    "    split_number=split_number,\n",
    "    input_shape=image_shape,\n",
    "    sequence_length=sequence_length,\n",
    "    training=False\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Classification criterion\n",
    "cls_criterion = nn.CrossEntropyLoss().to(device)\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvLSTM(\n",
      "  (encoder): Encoder(\n",
      "    (feature_extractor): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (6): ReLU(inplace=True)\n",
      "        (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (8): ReLU(inplace=True)\n",
      "        (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (11): ReLU(inplace=True)\n",
      "        (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (13): ReLU(inplace=True)\n",
      "        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (15): ReLU(inplace=True)\n",
      "        (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (18): ReLU(inplace=True)\n",
      "        (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (20): ReLU(inplace=True)\n",
      "        (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (22): ReLU(inplace=True)\n",
      "        (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (25): ReLU(inplace=True)\n",
      "        (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (27): ReLU(inplace=True)\n",
      "        (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (29): ReLU(inplace=True)\n",
      "        (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (1): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    )\n",
      "    (final): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (lstm): LSTM(\n",
      "    (lstm): LSTM(256, 1024, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (output_layers): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=1024, out_features=49, bias=True)\n",
      "    (4): Softmax(dim=-1)\n",
      "  )\n",
      "  (attention_layer): Linear(in_features=2048, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define network\n",
    "model = ConvLSTM(\n",
    "    num_classes=train_dataset.num_classes,\n",
    "    latent_dim=latent_dim,\n",
    "    lstm_layers=1,\n",
    "    hidden_dim=1024,\n",
    "    bidirectional=True,\n",
    "    attention=True,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "# summary(model, input_size=(50, 3, 224, 224), batch_size=4)\n",
    "        \n",
    "        \n",
    "# Add weights from checkpoint model if specified\n",
    "if checkpoint_model:\n",
    "    model.load_state_dict(torch.load(opt.checkpoint_model))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(epoch):\n",
    "    \"\"\" Evaluate the model on the test set \"\"\"\n",
    "    model.eval()\n",
    "    test_metrics = {\"loss\": [], \"acc\": []}\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    for images, labels in tqdm(test_dataloader):\n",
    "        image_sequences = Variable(images.to(device), requires_grad=False)\n",
    "        labels = Variable(labels, requires_grad=False).to(device)\n",
    "        with torch.no_grad():\n",
    "            # Reset LSTM hidden state\n",
    "            model.lstm.reset_hidden_state()\n",
    "            # Get sequence predictions\n",
    "            predictions = model(image_sequences)\n",
    "        # Compute metrics\n",
    "        acc = (predictions.detach().argmax(1) == labels).cpu().numpy().mean()\n",
    "        loss = cls_criterion(predictions, labels).item()\n",
    "        # Keep track of loss and accuracy\n",
    "        test_loss += loss\n",
    "        test_acc += acc\n",
    "\n",
    "    model.train()\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logfile = 'log.txt'\n",
    "def train(num_epochs, model, optimizer):\n",
    "    epoch_metrics = {\"loss\": [], \"acc\": []}\n",
    "    test_metrics = {\"loss\": [], \"acc\": []}\n",
    "    \n",
    "     # initialize the early_stopping object\n",
    "#     early_stopping = EarlyStopping(patience=2, verbose=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        prev_time = time.time()\n",
    "        with open(logfile, 'a') as log: \n",
    "            log.write(f\"--- Epoch {epoch} ---\\n\")\n",
    "            log.close()\n",
    "        print(f\"--- Epoch {epoch+1} ---\")\n",
    "        \n",
    "        train_loss = 0\n",
    "        steps = 0\n",
    "        for images, labels in tqdm(train_dataloader):\n",
    "\n",
    "            if images.size(0) == 1:\n",
    "                continue\n",
    "\n",
    "            image_sequences = Variable(images.to(device), requires_grad=True)\n",
    "            labels = Variable(labels.to(device), requires_grad=False)\n",
    "            steps += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Reset LSTM hidden state\n",
    "            model.lstm.reset_hidden_state()\n",
    "\n",
    "            # Get sequence predictions\n",
    "            predictions = model(image_sequences)\n",
    "\n",
    "            # Compute metrics\n",
    "            loss = cls_criterion(predictions, labels)\n",
    "            acc = 100 * (predictions.detach().argmax(1) == labels).cpu().numpy().mean()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Keep track of epoch metrics\n",
    "            train_loss += loss.item()\n",
    "\n",
    "\n",
    "            # Empty cache\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        test_loss, test_acc = test_model(epoch)\n",
    "        epoch_metrics[\"loss\"].append(train_loss/steps)\n",
    "        test_metrics[\"loss\"].append(test_loss/len(test_dataloader))\n",
    "        test_metrics[\"acc\"].append(test_acc/len(test_dataloader))\n",
    "        \n",
    "        # early_stopping needs the validation loss to check if it has decresed, \n",
    "        # and if it has, it will make a checkpoint of the current model\n",
    "#         early_stopping(test_metrics[\"loss\"], model)\n",
    "        \n",
    "#         if early_stopping.early_stop:\n",
    "#             print(\"Early stopping\")\n",
    "\n",
    "        with open(logfile, 'a') as log: \n",
    "            log.write(\"Epoch: {}/{} - \".format(epoch+1, num_epochs))\n",
    "            log.write(\"Training Loss: {:.3f}\".format(epoch_metrics[\"loss\"][-1]))\n",
    "            log.write(\"Test Loss: {:.3f}\".format(test_metrics[\"loss\"][-1]))\n",
    "            log.write(\"Test Acc: {:.3f}\".format(test_metrics[\"acc\"][-1]))\n",
    "\n",
    "        print(\"Epoch: {}/{} - \".format(epoch+1, num_epochs),\n",
    "              \"Training Loss: {:.3f}\".format(epoch_metrics[\"loss\"][-1]),\n",
    "              \"Test Loss: {:.3f}\".format(test_metrics[\"loss\"][-1]),\n",
    "              \"Test Acc: {:.3f}\".format(test_metrics[\"acc\"][-1]))\n",
    "\n",
    "        # Save model checkpoint\n",
    "        if epoch % checkpoint_interval == 0:\n",
    "            os.makedirs(\"model_checkpoints\", exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"model_checkpoints/{model.__class__.__name__}_{epoch}.pth\")\n",
    "            \n",
    "    return epoch_metrics, test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph_epoch(epoch_metrics, test_metrics, epoch):\n",
    "    plt.title(\"{} Epoch Training/Validation losses over epoch\".format(epoch))\n",
    "    plt.plot(epoch_metrics['loss'], label = \"Training Loss\")\n",
    "    plt.plot(test_metrics[\"loss\"], label = 'Testing Loss')\n",
    "    plt.xlabel('Number of epoch', fontsize = 14)\n",
    "    plt.ylabel('Loss', fontsize = 14)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1177\n"
     ]
    }
   ],
   "source": [
    "print(len(epoch_metrics[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1177 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1177/1177 [25:28<00:00,  1.30s/it]\n",
      "100%|██████████| 470/470 [08:14<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15 -  Training Loss: 3.749 Test Loss: 3.602 Test Acc: 0.399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1177 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1177/1177 [25:30<00:00,  1.30s/it]\n",
      "100%|██████████| 470/470 [07:24<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/15 -  Training Loss: 3.490 Test Loss: 3.457 Test Acc: 0.555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1177 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 524/1177 [09:33<14:31,  1.33s/it]"
     ]
    }
   ],
   "source": [
    "epoch_metrics, test_metrics = train(num_epochs, model, optimizer)\n",
    "\n",
    "plt.figure()\n",
    "plot_graph_epoch(epoch_metrics, test_metrics,  10)\n",
    "plt.figure()\n",
    "plt.title(\"Testing Accuracy\")\n",
    "plt.plot(test_metrics[\"acc\"], label=\"Testing Acc\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}